{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Assignment 01\n",
    "This notebook consists of five combined exercises from L01 and L02, which is required for assignment O1.\n",
    "\n",
    "## L01 - Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Below code is the code used to generate some of the figures in chapter 1._\n",
    "\n",
    "Afterwards a list of questions will be answered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"fundamentals\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"IGNORING: Saving figure\", fig_id) # SWMAL: I've disabled saving of figures\n",
    "    #if tight_layout:\n",
    "    #    plt.tight_layout()\n",
    "    #plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_country_stats(oecd_bli, gdp_per_capita):\n",
    "    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "    gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n",
    "                                  left_index=True, right_index=True)\n",
    "    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "    keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datapath = os.path.join(\"datasets\", \"lifesat\", \"\")\n",
    "\n",
    "# NOTE: a ! prefix makes us able to run system commands..\n",
    "# (command 'dir' for windows, 'ls' for Linux or Macs)\n",
    "#\n",
    "\n",
    "! dir\n",
    "\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "    gdp_per_capita = pd.read_csv(datapath + \"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "except Exception as e:\n",
    "    print(f\"SWMAL NOTE: well, you need to have the 'datasets' dir in path, please unzip 'datasets.zip' and make sure that its included in the datapath='{datapath}' setting in the cell above..\")\n",
    "    raise e\n",
    "    \n",
    "# Prepare the data\n",
    "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "\n",
    "# Select a linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make a prediction for Cyprus\n",
    "X_new = [[22587]]  # Cyprus' GDP per capita\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred) # outputs [[ 5.96242338]]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "#oecd_bli.head(2)\n",
    "\n",
    "gdp_per_capita = pd.read_csv(datapath+\"gdp_per_capita.csv\", thousands=',', delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "#gdp_per_capita.head(2)\n",
    "\n",
    "full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True)\n",
    "full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "#full_country_stats\n",
    "\n",
    "remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "\n",
    "sample_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "#missing_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[remove_indices]\n",
    "\n",
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "position_text = {\n",
    "    \"Hungary\": (5000, 1),\n",
    "    \"Korea\": (18000, 1.7),\n",
    "    \"France\": (29000, 2.4),\n",
    "    \"Australia\": (40000, 3.0),\n",
    "    \"United States\": (52000, 3.8),\n",
    "}\n",
    "for country, pos_text in position_text.items():\n",
    "    pos_data_x, pos_data_y = sample_data.loc[country]\n",
    "    country = \"U.S.\" if country == \"United States\" else country\n",
    "    plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text,\n",
    "            arrowprops=dict(facecolor='black', width=0.5, shrink=0.1, headwidth=5))\n",
    "    plt.plot(pos_data_x, pos_data_y, \"ro\")\n",
    "#save_fig('money_happy_scatterplot')\n",
    "plt.show()\n",
    "\n",
    "from sklearn import linear_model\n",
    "lin1 = linear_model.LinearRegression()\n",
    "Xsample = np.c_[sample_data[\"GDP per capita\"]]\n",
    "ysample = np.c_[sample_data[\"Life satisfaction\"]]\n",
    "lin1.fit(Xsample, ysample)\n",
    "\n",
    "t0 = 4.8530528\n",
    "t1 = 4.91154459e-05\n",
    "\n",
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "M=np.linspace(0, 60000, 1000)\n",
    "plt.plot(M, t0 + t1*M, \"b\")\n",
    "plt.text(5000, 3.1, r\"$\\theta_0 = 4.85$\", fontsize=14, color=\"b\")\n",
    "plt.text(5000, 2.2, r\"$\\theta_1 = 4.91 \\times 10^{-5}$\", fontsize=14, color=\"b\")\n",
    "#save_fig('best_fit_model_plot')\n",
    "plt.show()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) The $\\theta$ parameters and the $R^2$ Score\n",
    " \n",
    "The $\\theta$ parameters, also refered to as weights or coefficients, can be adjusted during training to optimize the performance and acurracy of a certain linear regression model. \n",
    "\n",
    "Extrating the $\\theta_0$ and $\\theta_1$ coefficients can be done by using the below python attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = model.intercept_\n",
    "theta_1 = model.coef_[0]\n",
    "\n",
    "print(\"Theta_0 (Intercept):\", theta_0)\n",
    "print(\"Theta_1 (Coefficient):\", theta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Another important variable is the $R^2$ score. \n",
    "\n",
    "The $R^2$ measures the goodness of fit of the model itself. The minimum and maximum values range from 0 to 1, where the below list describes each outcome:\n",
    "\n",
    "- A value of 1: Considered a perfect fit, where the model accurately predicts the dependent variable based on the independent variables (Explains all the variance).\n",
    "- A value of 0: The opposite of a perfect fit. This score indicates that the model's prediction is entirely wrong and provides no explanation.\n",
    "\n",
    "While it is desirable to achieve a score of 1, it's important to note that such a score is highly unlikely as it's almost impossible to create a model that makes a perfect fit to real-world data.\n",
    "\n",
    "Therefore, it is preferable to find a value close to 1, which, as explained, indicates a better fit.\n",
    "\n",
    "In the assignment, the below calculation extracts the desirable score of 0.734.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(X, y) \n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Using k-Nearest Neighbors\n",
    "\n",
    "Instead of using linear regression, it will now be a k-nearest neighbour algorithm, here with k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data\n",
    "country_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Prepare the data\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "print(\"X.shape=\",X.shape)\n",
    "print(\"y.shape=\",y.shape)\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Select and train a model\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# Train the new model\n",
    "knn.fit(X, y) \n",
    "\n",
    "\n",
    "# Predictions for Cyprus \n",
    "print(\"KNN prediction: \", knn.predict(X_new))  \n",
    "print(\"Linear regression prediction: \", lin1.predict(X_new))  \n",
    "\n",
    "# Evaluating the models \n",
    "print(\"KKN score: \", knn.score(X, y))\n",
    "print(\"Linear score: \", lin1.score(X, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As seen in the calculations above, the KNN prediction for Cyprus is 5.77, which is a bit less than the linear prediction of 5.96. \n",
    "\n",
    "KNN uses the score method of the coefficient of determination (R^2), as does the linear regression model. This means they are able to be compared, since they use the same scoring methods. \n",
    "\n",
    "\n",
    "The model used here is the \"Regression based on k-nearest neighbors\" (URL: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc) Tuning Parameter for k-Nearest Neighbors and A Sanity Check\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "But that not the full story. Try plotting the prediction for both models in the same graph and tune the `k_neighbor` parameter of the `KNeighborsRegressor` model.  \n",
    "\n",
    "Choosing `k_neighbor=1` produces a nice `score=1`, that seems optimal...but is it really so good?\n",
    "- No, when score=1 it means that the model fits perfectly. Though this is true in some sense, it gives an obscured view. When k_neighbor=1 the model only considers a single nearest neighboor. Then the model doesn't consider the outliers in the data, and then we get a fragile model.  \n",
    "\n",
    "Plotting the two models in a 'Life Satisfaction-vs-GDP capita' 2D plot by creating an array in the range 0 to 60000 (USD) (the `M` matrix below) and then predict the corresponding y value will sheed some light to this. \n",
    "\n",
    "Now reusing the plots stubs below, try to explain why the k-nearest neighbour with `k_neighbor=1` has such a good score.\n",
    "- When there's only one neighbor, it will go directly from point to point. This will give the illusion that the model fits perfectly because it only has to consider 1 neighbor. On the other hand, when the model has to consider 3 neighbors, it tries to go to all of them, which is not possible. Then it makes a mean of the data. \n",
    "\n",
    "Does a score=1 with `k_neighbor=1`also mean that this would be the prefered estimator for the job?\n",
    "- No, when the score is 1, it suggests that there's something wrong with the model or the way that the model is used. In this scenario, when the neighbor=1, the score is 1. This suggests that the model doesn't work as intended. There could be a lot of reasons why this is the case, but in this instance, it's because the neighbor=1.\n",
    "\n",
    "Hint here is a similar plot of a KNN for a small set of different k's:\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L01/Figs/regression_with_knn.png\"  alt=\"WARNING: could not get image from server.\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "\n",
    "# create an test matrix M, with the same dimensionality as X, and in the range [0;60000] \n",
    "# and a step size of your choice\n",
    "m=np.linspace(0, 60000, 1000)\n",
    "M=np.empty([m.shape[0],1])\n",
    "M[:,0]=m\n",
    "\n",
    "# from this test M data, predict the y values via the lin.reg. and k-nearest models\n",
    "y_pred_lin = lin1.predict(M)\n",
    "y_pred_knn = knn.predict(M)   # ASSUMING the variable name 'knn' of your KNeighborsRegressor \n",
    "\n",
    "# use plt.plot to plot x-y into the sample_data plot..\n",
    "plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_knn, \"b\")\n",
    " \n",
    "# Select and train a model with neighbor 1\n",
    "knn1 = KNeighborsRegressor(n_neighbors=1)\n",
    "knn1.fit(X, y) \n",
    "y_pred_knn1 = knn1.predict(M)    \n",
    "plt.plot(m, y_pred_knn1, \"g\")\n",
    "print(knn1.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Trying out a Neural Network\n",
    "\n",
    "Let us then try a Neural Network on the data, using the fit-predict interface allows us to replug a new model into our existing code.\n",
    "\n",
    "There are a number of different NN's available, let's just hook into Scikit-learns Multi-Layer Perceptron for regression, that is an 'MLPRegressor'. \n",
    "\n",
    "Now, the data-set for training the MLP is really not well scaled, so we need to tweak a lot of parameters in the MLP just to get it to produce some sensible output: with out preprocessing and scaling of the input data, `X`, the MLP is really a bad choice of model for the job since it so easily produces garbage output. \n",
    "\n",
    "Try training the `mlp` regression model below, predict the value for Cyprus, and find the `score` value for the training set...just as we did for the linear and KNN models.\n",
    "\n",
    "Can the `MLPRegressor` score function be compared with the linear and KNN-scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Setup MLPRegressor\n",
    "mlp = MLPRegressor( hidden_layer_sizes=(10,), solver='adam', activation='relu', tol=1E-5, max_iter=100000, verbose=True)\n",
    "mlp.fit(X, y.ravel())\n",
    "\n",
    "# lets make a MLP regressor prediction and redo the plots\n",
    "y_pred_mlp = mlp.predict(M) \n",
    "\n",
    "plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_knn, \"b\")\n",
    "plt.plot(m, y_pred_mlp, \"k\")\n",
    "\n",
    "print(mlp.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try training the `mlp` regression model below, predict the value for Cyprus, and find the `score` value for the training set...just as we did for the linear and KNN models.\n",
    "\n",
    "We have trained the mlp model, which changes each time the code is run because it's trained by a neural network now.\n",
    "The neural network stops the training when it doesn't improve more than tol=0.000010 for 10 consecutive epochs.\n",
    "\n",
    "\n",
    "### Can the `MLPRegressor` score function be compared with the linear and KNN-scores?\n",
    "They can be compared since the score function is implemented in the same way. The result is way different from the score results of the linear and knn scores, which in this case means that the mlp model is worse at the prediction since the result (R^2) is negative. This means that the predicted values are performing worse than if you where to take the average value of the data as a prediction. (Læs lige op på det her?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L01 - Modules and Packages in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qa) Load and test the `libitmal` module\n",
    "\n",
    "Here, we import the libitmal module from utils, which was given in the assignment. We print its directory and its content, and lastly call its TestAll() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from libitmal import utils as itmalutils\n",
    "\n",
    "print(dir(itmalutils))\n",
    "print(itmalutils.__file__)\n",
    "\n",
    "itmalutils.TestAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb) Create your own module, with some functions, and test it\n",
    "\n",
    "Here we will import our own module named `extraUtils`. The module is imported from `coolModule`, where its functions now becomes available.\n",
    "\n",
    "In this example, the `printNogetSejt()` is used to print a predfined text along with a given parameter string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraUtils import coolModule as module\n",
    "\n",
    "module.printNogetSejt(\"Thomas\")\n",
    "\n",
    "print(module.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc) How do you 'recompile' a module?\n",
    "\n",
    "When changing the module code, Jupyter will keep running on the old module. How do you force the Jupyter notebook to re-load the module changes? \n",
    "\n",
    "You can force a reload of a module by using the `reload()` function from the `importlib` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from extraUtils import coolModule as module   \n",
    "\n",
    "reload(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to automatically reload modules before executing code by using magic commands such as `%load_ext` and `%autoreload`\n",
    "\n",
    "Source: https://saturncloud.io/blog/jupyter-notebook-reload-module-a-comprehensive-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qe) Extend the class with some public and private functions and member variables \n",
    "\n",
    "\n",
    "Below, the class MyClass has been defined with both a private and a public function. \n",
    "\n",
    "Private funtions are defined by the two prefixed underscores (__), while member variables are defined by the \"self.\" annotation. This self in python classes represents an instance of the class, therefor making it possible to access methods and attributes inside the class itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    # Public function \n",
    "    def myfun(self):\n",
    "        # Member variable \n",
    "        self.myvar = \"blah\"\n",
    "        print(f\"This is a message inside the class, myvar={self.myvar}.\")\n",
    "        \n",
    "        # Private function \n",
    "    def __private_function(self):\n",
    "        self.asdf = \"asdf\"\n",
    "        print(f\"This is a private function, {self.asdf}.\")\n",
    "        \n",
    "    def something():\n",
    "        print(\"Something\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an instance of the class is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myObjectx = MyClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to call a function with the self parameter missing, it throws an error, because the program is passing it an instance of a class, but it doesn't take one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    myObjectx.something()\n",
    "except:\n",
    "    print(\"Self should be added to the implementation of something() if you want to call it this way\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to call the private function, it fails because there is no function available called __private_function() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    myObjectx.__private_function() \n",
    "except:\n",
    "    print(\"No function called __private_function()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one wants to use a method with a self parameter, one needs to call it with the class directly and not an instance of the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyClass.something() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qf) Extend the class with a Constructor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is shown how to define a constructor in a Python class, namely the \\_\\_init\\_\\_ function. \n",
    "This function will be called every time a class is instantiated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myClass:\n",
    "    def __init__(self):\n",
    "        self.att1 = 2\n",
    "        self.att2 = 3\n",
    "        self.att3 = 4\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as a constructor, it is also possible to make a destructor in a Python class. Below it is shown how this \\_\\_del\\_\\_ function is defined in a class. \n",
    "\n",
    "\n",
    "The destructor function is not necessary in Python, since the language has garbage collection which handles the memory management automatically. \n",
    "So a detructor is not strictly necessary, except if you want to handle the memory management yourself.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myClass: \n",
    "    def __del__(self):\n",
    "        print(\"The object has been deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the destructor on an instance of the class, it will delete the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myobejctx = myClass() \n",
    "del myobejctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qg) Extend the class with a to-string function\n",
    "\n",
    "\n",
    "... \n",
    "MISSING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L02 - Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qa) Given the following $\\mathbf{x}^{(i)}$'s, construct and print the $\\mathbf X$ matrix in python.\n",
    "\n",
    "Using numpy the design matrix is being constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array([1,2,3,4]) \n",
    "\n",
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([4,2,1])\n",
    "x3 = np.array([3,8,5])\n",
    "x4 = np.array([-9,-1,0])\n",
    "\n",
    "X = np.array([x1, x2, x3, x4])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb) Implement the L1 and L2 norms for vectors in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The norm functions `L1` and `L2` are being implemented, where we assert and validate the output of the function. The functions will be used later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def checkInputIsOneDimensional(x):\n",
    "    assert x.shape[0]>=0 \n",
    "    if not x.ndim==1:\n",
    "        raise ValueError\n",
    "    \n",
    "def checkInputSameShape(y_pred, y_true):\n",
    "    assert y_pred.shape == y_true.shape, \"Shape of input is not equal!\"\n",
    "\n",
    "def L1(x):\n",
    "    checkInputIsOneDimensional(x) #Check if parameter is good\n",
    "    sum = 0\n",
    "    for i in x:\n",
    "        sum = sum + (i**2)**0.5  \n",
    "    return sum\n",
    "\n",
    "def L2(x):\n",
    "    checkInputIsOneDimensional(x) #Check if parameter is good\n",
    "    sum = 0\n",
    "    for i in x:\n",
    "        sum = sum + (i**2)\n",
    "    sum = sum**0.5\n",
    "    return sum\n",
    "\n",
    "def L2Dot(x):\n",
    "    checkInputIsOneDimensional(x) #Check if parameter is good\n",
    "    sum = np.sqrt(np.dot(x,x))  \n",
    "    return sum  \n",
    "     \n",
    "tx=np.array([1, 2, 3, -1])\n",
    "ty=np.array([3,-1, 4,  1])\n",
    "\n",
    "expected_d1=8.0\n",
    "expected_d2=4.242640687119285\n",
    "\n",
    "d1=L1(tx-ty)\n",
    "d2=L2(tx-ty)\n",
    "\n",
    "print(f\"tx-ty={tx-ty}, d1-expected_d1={d1-expected_d1}, d2-expected_d2={d2-expected_d2}\")\n",
    "\n",
    "eps=1E-9  \n",
    "assert math.fabs(d1-expected_d1)<eps, \"L1 dist seems to be wrong\" \n",
    "assert math.fabs(d2-expected_d2)<eps, \"L2 dist seems to be wrong\" \n",
    "\n",
    "print(\"OK(part-1)\")\n",
    " \n",
    "d2dot=L2Dot(tx-ty)\n",
    "print(\"d2dot-expected_d2=\",d2dot-expected_d2)\n",
    "assert math.fabs(d2dot-expected_d2)<eps, \"L2Ddot dist seem to be wrong\" \n",
    "print(\"OK(part-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc) Construct the Root Mean Square Error (RMSE) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, y_true):\n",
    "    checkInputSameShape(y_pred, y_true)\n",
    "    sum = L2(y_pred - y_true)*0.5 #Implementation of the RMSE\n",
    "    return sum\n",
    "\n",
    "# Dummy h function:\n",
    "def h(X):    \n",
    "    if X.ndim!=2:\n",
    "        raise ValueError(\"excpeted X to be of ndim=2, got ndim=\",X.ndim)\n",
    "    if X.shape[0]==0 or X.shape[1]==0:\n",
    "        raise ValueError(\"X got zero data along the 0/1 axis, cannot continue\")\n",
    "    return X[:,0]\n",
    " \n",
    "r=RMSE(h(X),y)\n",
    "\n",
    "# TEST vector:\n",
    "eps=1E-9\n",
    "expected=6.57647321898295\n",
    "print(f\"RMSE={r}, diff={r-expected}\")\n",
    "assert math.fabs(r-expected)<eps, \"your RMSE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qd) Similar construct the Mean Absolute Error (MAE) function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created the `RMSE` function in the previous task, and now we will implement the MAE function to calculate the norm of L1 (instead of L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_pred, y_true):\n",
    "    checkInputSameShape(y_pred, y_true)\n",
    "    sum = L1(y_pred-y_true)/y_pred.size\n",
    "    return sum  \n",
    "\n",
    "r=MAE(h(X), y)\n",
    "\n",
    "# TEST vector:\n",
    "expected=3.75\n",
    "print(f\"MAE={r}, diff={r-expected}\")\n",
    "assert math.fabs(r-expected)<eps, \"MAE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qe) Robust Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added error checking code by using assertions, that checks for right y^-y sizes of the MSE and MAE functions (the shape of their input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qf) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L02 - Dummy classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qa) Load and display the MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function MNIST_GetDataSet() will be implemented by using the fetch_openml function from sklearn.datasets. The parameters of this function will be found by looking at the documentation https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html.\n",
    "\n",
    "The parameters that is needed to be changed from default values:\n",
    "\n",
    "`name: str` -> \n",
    "\n",
    "`version: int` ->\n",
    "\n",
    "`return_X_y: bool` ->\n",
    "\n",
    "`cache: bool` ->\n",
    "\n",
    "`as_frame: bool` -> We are using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MNIST_GetDataSet():\n",
    "    X, y = fetch_openml(name='mnist_784', version=1, data_home=\"./data\", cache=True, return_X_y = True, as_frame=False)\n",
    "    X = X/255\n",
    "    return X,y\n",
    "\n",
    "def MNIST_PlotDigit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "X,y = MNIST_GetDataSet()\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# Sejt -- y[2] er 4 og x[2] er også 4 hehe\n",
    "MNIST_PlotDigit(X[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb)  Add a Stochastic Gradient Decent [SGD] Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "y_train_5 = (y_train == \"5\")\n",
    "y_test_5 = (y_test == \"5\")\n",
    "\n",
    "SGD_classifier = SGDClassifier(random_state=42)\n",
    "\n",
    "SGD_classifier.fit(X_train, y_train_5)\n",
    "\n",
    "y_pred = SGD_classifier.predict(X_test)\n",
    "\n",
    "categorized = np.where(y_pred == y_test_5)[0]\n",
    "\n",
    "not_correctly_categorized = np.where(y_pred != y_test_5)[0]\n",
    "\n",
    "print(\"\\n---- Correct categorized fives:\")\n",
    "\n",
    "for number in categorized[:2]:\n",
    "    MNIST_PlotDigit(X_test[number])\n",
    "    predicted_value = SGD_classifier.predict([X_test[number]])\n",
    "    print(f\"Is it a 5?: {predicted_value[0]}\")\n",
    "    plt.show()\n",
    "    \n",
    "print(\"\\n---- Incorrect categorized fives\")\n",
    "\n",
    "for number in not_correctly_categorized[:2]:\n",
    "    MNIST_PlotDigit(X_test[number])\n",
    "    predicted_value = SGD_classifier.predict([X_test[number]])\n",
    "    print(f\"Is it a 5?: {predicted_value[0]}\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc) Implement a dummy binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class DummyClassifier():\n",
    "    def fit(self, X, y=None):\n",
    "        pass # Dummy classifier doesn't fit\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [0] * len(X) # Writes 0 in every index\n",
    "    \n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "dummyClassifier = DummyClassifier()\n",
    "\n",
    "dummyClassifier.fit(X_train)\n",
    "\n",
    "y_pred = dummyClassifier.predict(X_test)\n",
    "\n",
    "accuracy = calculate_accuracy(y_test_5, y_pred)\n",
    "\n",
    "print(accuracy) # The accuracy will be around 90%, this is a fake accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qd) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L02 - Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Qa) Implement the Accuracy function and test it on the MNIST data.\n",
    "\n",
    "The accuaracy function is going to be created as MyAccuracy(), where the following formula will be implemented by using for loops to find the total amount  of true positives, true negatives and then divide it by the total amount of data points:\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "      a &= \\myfrac{TP + TN}{TP + TN + FP + FN}\\\\\n",
    "  }\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def MNIST_GetDataSet():\n",
    "    X, y = fetch_openml(name='mnist_784', version=1, data_home=\"./data\", cache=True, return_X_y = True, as_frame=False)\n",
    "    X = X/255\n",
    "    return X,y\n",
    "\n",
    "class DummyClassifier():\n",
    "    def fit(self, X, y=None):\n",
    "        pass # Dummy classifier doesn't fit\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [0] * len(X) # Writes 0 in every index\n",
    "\n",
    "# MyAccuracy function that calculates the accuracy\n",
    "def MyAccuracy(y_true, y_pred):\n",
    "    if len(y_true) == 0 or len(y_pred) == 0: # Error handling\n",
    "        return 0\n",
    "    matches = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            matches += 1\n",
    "    return(matches/len(y_pred))\n",
    "    \n",
    "# TEST FUNCTION: example of a comperator, using Scikit-learn accuracy_score\n",
    "def TestAccuracy(y_true, y_pred):\n",
    "   a0=MyAccuracy(y_true, y_pred)\n",
    "   a1=accuracy_score(y_true, y_pred)\n",
    "\n",
    "   print(f\"my a          ={a0}\")\n",
    "   print(f\"scikit-learn a={a1}\")\n",
    "\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "y_train_5 = (y_train == \"5\")\n",
    "y_test_5 = (y_test == \"5\")\n",
    "\n",
    "SGD_classifier = SGDClassifier(random_state=42)\n",
    "\n",
    "SGD_classifier.fit(X_train, y_train_5)\n",
    "\n",
    "y_pred = SGD_classifier.predict(X_test)\n",
    "\n",
    "dummyClassifier = DummyClassifier()\n",
    "\n",
    "dummyClassifier.fit(X_train)\n",
    "\n",
    "dummy_y_pred = dummyClassifier.predict(X_test)\n",
    "\n",
    "# Test accuracy of dummy\n",
    "print(\"\\nAccuracy of dummy classifier:\")\n",
    "TestAccuracy(y_test_5, dummy_y_pred)\n",
    "\n",
    "#Test accuracy of SGD_classifiet\n",
    "print(\"\\nAccuracy of SGD classifier:\")\n",
    "TestAccuracy(y_test_5, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb) Implement Precision, Recall and $F_1$-score and test it on the MNIST data for both the SGD and Dummy classifier models\n",
    "\n",
    "The precision will implement the following formula by using two for loops to extract the amount of true positives and the false positives:\n",
    "$$\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bM{\\mathbf{M}}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\def\\st#1{_{\\scriptsize\\textrm{#1}}}\n",
    "    \\def\\myfrac#1#2{\\frac{#1\\rule{0pt}{8pt}}{#2\\rule{0pt}{8pt}}} \n",
    "\\ar{rl}{\n",
    "  p &= \\myfrac{TP}{TP + FP}\n",
    "}\n",
    "$$\n",
    "\n",
    "The recall will implement the following formula by again using two for loops, but this time extracting the amount of true positives and the false negatives:\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "    r &= \\myfrac{TP}{TP + FN}\\\\\n",
    "  }\n",
    "$$\n",
    "\n",
    "Lastly the F1 score will implement the following formula, where the implemented versions of MyPrecision and MyRecall will be used:\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "    F_1 &= \\myfrac{2pr}{p+r}\\\\\n",
    "  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def MyPrecision(y_true, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == True and y_true[i] == True:\n",
    "            TP += 1\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == True and y_true[i] == False:\n",
    "            FP += 1\n",
    "\n",
    "    if TP == 0 and FP == 0: # Error handling cant divide by zero\n",
    "        return 0\n",
    "            \n",
    "    return(TP/(TP+FP))\n",
    "            \n",
    "\n",
    "def MyRecall(y_true, y_pred):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == True and y_true[i] == True:\n",
    "            TP += 1\n",
    "            \n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == False and y_true[i] == True:\n",
    "            FN += 1\n",
    "    if TP == 0 and FN == 0: # Error handling cant divide by zero\n",
    "        return 0\n",
    "    return(TP/(TP+FN))        \n",
    "    \n",
    "    \n",
    "def MyF1Score(y_true, y_pred):\n",
    "    precision = MyPrecision(y_true, y_pred)\n",
    "    recall = MyRecall(y_pred, y_true)\n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    return(2 * precision * recall)/(precision + recall)\n",
    "\n",
    "y_pred1 = [True, True, True, True]\n",
    "y_true1 = [True, True, True, False]\n",
    "\n",
    "y_pred2 = [True, True, False, True]\n",
    "y_true2 = [True, True, True, False]\n",
    "\n",
    "def test(y_true, y_pred):\n",
    "    p0 = MyPrecision(y_true, y_pred)\n",
    "    p1 = precision_score(y_true, y_pred)\n",
    "    \n",
    "    r0 = MyRecall(y_true, y_pred)\n",
    "    r1 = recall_score(y_true, y_pred)\n",
    "    \n",
    "    f0 = MyF1Score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"This is my precision: {p0}\")\n",
    "    print(f\"This is scikit precision: {p1}\")\n",
    "    \n",
    "    print(f\"This is my recall: {r0}\")\n",
    "    print(f\"This is scikit recall: {r1}\")\n",
    "    \n",
    "    print(f\"This is my f1-score: {f0}\")\n",
    "    print(f\"This is scikit f1-score: {f1}\\n\")\n",
    "\n",
    "print(\"First test case: \")\n",
    "test(y_true1, y_pred1)\n",
    "print(\"Second test case: \")\n",
    "test(y_true2, y_pred2)\n",
    "print(\"SGD test case: \")\n",
    "test(y_test_5, y_pred)\n",
    "print(\"Dummy test case: \")\n",
    "test(y_test_5, dummy_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc) The Confusion Matrix\n",
    "\n",
    "Now the confusion matrix' will be created by using the confusion_matrix function from sklearn.metrics. By reading the documentation of this function it is seen that the scikit-learn confusion matrix is organized in the following order TN, FP, FN, TP. Therefor it is also important not to mess up the parameters because it will mess up the organization of the confusion matrix, and the TN, FP, FN, TP will be swapped around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_dummy = confusion_matrix(y_test_5, dummy_y_pred)\n",
    "cm_sdg_classifier = confusion_matrix(y_test_5, y_pred)\n",
    "cm_sdg_classifier_reversed = confusion_matrix(y_pred, y_test_5)\n",
    "\n",
    "print(f\"Dummy maxtrix:\\n{cm_dummy}\\n\")\n",
    "print(f\"SDG matrix:\\n{cm_sdg_classifier}\\n\")\n",
    "print(f\"Reversed SDG matrix:\\n{cm_sdg_classifier_reversed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qd) A Confusion Matrix Heat-map\n",
    "\n",
    "The heat map will be created with the ConfusionMatrixDisplay function, where we can compare the output with the output from the Qc) The Confusion Matrix exercise, these should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "dummy_matrix = ConfusionMatrixDisplay(confusion_matrix=cm_dummy) \n",
    "dummy_matrix.plot()\n",
    "plt.show()\n",
    "\n",
    "SGD_matrix = ConfusionMatrixDisplay(confusion_matrix=cm_sdg_classifier)\n",
    "SGD_matrix.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qe) Conclusion\n",
    "\n",
    "We have now worked with different performance metrics that shows how \"good\" your machine learning model is. In the first exercise we implemented the accuracy function and tested it with our dummy classifier and our SGD classifier. From this it was shown that the accuracy can give a great score (around 90%) to a machine learning model (our dummy classifier) which was totaly wrong, therfore we should be carefull to conclude how great our model is by only using the accuracy as our score. Afterwards we implemented the precision, recall and F1-score as further ways of judging our models. Now our dummy classifier will be found out to be a bad model, as it scores zero in these tests, wheras our SGD classifier still deliveres good scores. Therefore it can be concluded that it is important to take multiple scores into account, when judging a machine learning model. Lastly we created and showed a confussion matrix with the dummy classifier and SGD classifier, which creates an overview of how your model is performing, by showing the TN, FP, FN and TP. By using this overview it is quickly seen that the dummy classifier is performing worse than the sgd classifier since it is not predicting any TP, but instead predicting alot of FN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
