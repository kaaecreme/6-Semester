{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Intro\n",
    "\n",
    "\n",
    "We startup by reusing parts of `01_the_machine_learning_landscape.ipynb` from Géron [GITHOML]. So we begin with what Géron says about life satisfactions vs GDP per capita.\n",
    " \n",
    "Halfway down this notebook, a list of questions for SWMAL is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 – The Machine Learning landscape\n",
    "\n",
    "_This is the code used to generate some of the figures in chapter 1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"fundamentals\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"IGNORING: Saving figure\", fig_id) # SWMAL: I've disabled saving of figures\n",
    "    #if tight_layout:\n",
    "    #    plt.tight_layout()\n",
    "    #plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_country_stats(oecd_bli, gdp_per_capita):\n",
    "    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "    gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n",
    "                                  left_index=True, right_index=True)\n",
    "    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "    keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datapath = os.path.join(\"datasets\", \"lifesat\", \"\")\n",
    "\n",
    "# NOTE: a ! prefix makes us able to run system commands..\n",
    "# (command 'dir' for windows, 'ls' for Linux or Macs)\n",
    "#\n",
    "\n",
    "! dir\n",
    "\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "    gdp_per_capita = pd.read_csv(datapath + \"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "except Exception as e:\n",
    "    print(f\"SWMAL NOTE: well, you need to have the 'datasets' dir in path, please unzip 'datasets.zip' and make sure that its included in the datapath='{datapath}' setting in the cell above..\")\n",
    "    raise e\n",
    "    \n",
    "# Prepare the data\n",
    "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "\n",
    "# Select a linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make a prediction for Cyprus\n",
    "X_new = [[22587]]  # Cyprus' GDP per capita\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred) # outputs [[ 5.96242338]]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "#oecd_bli.head(2)\n",
    "\n",
    "gdp_per_capita = pd.read_csv(datapath+\"gdp_per_capita.csv\", thousands=',', delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "#gdp_per_capita.head(2)\n",
    "\n",
    "full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True)\n",
    "full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "#full_country_stats\n",
    "\n",
    "remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "\n",
    "sample_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "#missing_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[remove_indices]\n",
    "\n",
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "position_text = {\n",
    "    \"Hungary\": (5000, 1),\n",
    "    \"Korea\": (18000, 1.7),\n",
    "    \"France\": (29000, 2.4),\n",
    "    \"Australia\": (40000, 3.0),\n",
    "    \"United States\": (52000, 3.8),\n",
    "}\n",
    "for country, pos_text in position_text.items():\n",
    "    pos_data_x, pos_data_y = sample_data.loc[country]\n",
    "    country = \"U.S.\" if country == \"United States\" else country\n",
    "    plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text,\n",
    "            arrowprops=dict(facecolor='black', width=0.5, shrink=0.1, headwidth=5))\n",
    "    plt.plot(pos_data_x, pos_data_y, \"ro\")\n",
    "#save_fig('money_happy_scatterplot')\n",
    "plt.show()\n",
    "\n",
    "from sklearn import linear_model\n",
    "lin1 = linear_model.LinearRegression()\n",
    "Xsample = np.c_[sample_data[\"GDP per capita\"]]\n",
    "ysample = np.c_[sample_data[\"Life satisfaction\"]]\n",
    "lin1.fit(Xsample, ysample)\n",
    "\n",
    "t0 = 4.8530528\n",
    "t1 = 4.91154459e-05\n",
    "\n",
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "M=np.linspace(0, 60000, 1000)\n",
    "plt.plot(M, t0 + t1*M, \"b\")\n",
    "plt.text(5000, 3.1, r\"$\\theta_0 = 4.85$\", fontsize=14, color=\"b\")\n",
    "plt.text(5000, 2.2, r\"$\\theta_1 = 4.91 \\times 10^{-5}$\", fontsize=14, color=\"b\")\n",
    "#save_fig('best_fit_model_plot')\n",
    "plt.show()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) The $\\theta$ parameters and the $R^2$ Score\n",
    " \n",
    "Extract the score=0.734 for the model using data (X,y) and explain what $R^2$ score measures in broad terms\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcll}\n",
    "    R^2 &=& 1 - u/v\\\\\n",
    "    u   &=& \\sum (y_{true} - y_{pred}~)^2   ~~~&\\small \\textrm{residual sum of squares}\\\\\n",
    "    v   &=& \\sum (y_{true} - \\mu_{true}~)^2 ~~~&\\small \\textrm{total sum of squares}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Hvad vi fandt ud af og hvad ska vi skrive på engelsk når vi skal lyde kloge**\n",
    "\n",
    " $\\theta_0$ = skæring på y-aksen\n",
    "\n",
    " $\\theta_1$ = hældningskoefficienten\n",
    "\n",
    " Score / Forklaringsgraden = Hvor godt lortet passer på. Min 0, maks 1. Kæmpe skidt hvis den er 1, så der noget galt. Så gerne tæt på 1, men ikke helt deroppe, for så passer den perfekt og det gør det fandme aldrig. En fitness/goodness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = model.intercept_\n",
    "theta_1 = model.coef_[0]\n",
    "score = model.score(X, y)\n",
    "\n",
    "print(\"Theta_0 (Intercept):\", theta_0)\n",
    "print(\"Theta_1 (Coefficient):\", theta_1)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Using k-Nearest Neighbors\n",
    "\n",
    "Instead of using linear regression, it will now be a k-nearest neighbour algorithm, here with k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data\n",
    "country_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Prepare the data\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "print(\"X.shape=\",X.shape)\n",
    "print(\"y.shape=\",y.shape)\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Select and train a model\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# Train the new model\n",
    "knn.fit(X, y) \n",
    "\n",
    "\n",
    "# Predictions for Cyprus \n",
    "print(\"KNN prediction: \", knn.predict(X_new))  \n",
    "print(\"Linear regression prediction: \", lin1.predict(X_new))  \n",
    "\n",
    "# Evaluating the models \n",
    "print(\"KKN score: \", knn.score(X, y))\n",
    "print(\"Linear score: \", lin1.score(X, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As seen in the calculations above, the KNN prediction for Cyprus is 5.77, which is a bit less than the linear prediction of 5.96. \n",
    "\n",
    "KNN uses the score method of the coefficient of determination (R^2), as does the linear regression model. This means they are able to be compared, since they use the same scoring methods. \n",
    "\n",
    "\n",
    "The model used here is the \"Regression based on k-nearest neighbors\" (URL: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc) Tuning Parameter for k-Nearest Neighbors and A Sanity Check\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "But that not the full story. Try plotting the prediction for both models in the same graph and tune the `k_neighbor` parameter of the `KNeighborsRegressor` model.  \n",
    "\n",
    "Choosing `k_neighbor=1` produces a nice `score=1`, that seems optimal...but is it really so good?\n",
    "- NEJ\n",
    "\n",
    "Plotting the two models in a 'Life Satisfaction-vs-GDP capita' 2D plot by creating an array in the range 0 to 60000 (USD) (the `M` matrix below) and then predict the corresponding y value will sheed some light to this. \n",
    "\n",
    "Now reusing the plots stubs below, try to explain why the k-nearest neighbour with `k_neighbor=1` has such a good score.\n",
    "- Når der kun er en nabo, så vil den altid ramme stop on på hver eneste datapoint. Kontra billedet der har tre neighbors, forsøger den så vidt muligt at ramme alle punkter, men det kan den jo ikke. Det kan den godt, hvis den kun skal gå 1 frem og tilbage.\n",
    "\n",
    "Does a score=1 with `k_neighbor=1`also mean that this would be the prefered estimator for the job?\n",
    "- NO\n",
    "\n",
    "Hint here is a similar plot of a KNN for a small set of different k's:\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L01/Figs/regression_with_knn.png\"  alt=\"WARNING: could not get image from server.\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "\n",
    "# create an test matrix M, with the same dimensionality as X, and in the range [0;60000] \n",
    "# and a step size of your choice\n",
    "m=np.linspace(0, 60000, 1000)\n",
    "M=np.empty([m.shape[0],1])\n",
    "M[:,0]=m\n",
    "\n",
    "# from this test M data, predict the y values via the lin.reg. and k-nearest models\n",
    "y_pred_lin = lin1.predict(M)\n",
    "y_pred_knn = knn.predict(M)   # ASSUMING the variable name 'knn' of your KNeighborsRegressor \n",
    "\n",
    "# use plt.plot to plot x-y into the sample_data plot..\n",
    "plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_knn, \"b\")\n",
    " \n",
    "# Select and train a model with neighbor 1\n",
    "knn1 = KNeighborsRegressor(n_neighbors=1)\n",
    "knn1.fit(X, y) \n",
    "y_pred_knn1 = knn1.predict(M)    \n",
    "plt.plot(m, y_pred_knn1, \"g\")\n",
    "print(knn1.score(X, y))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Trying out a Neural Network\n",
    "\n",
    "Let us then try a Neural Network on the data, using the fit-predict interface allows us to replug a new model into our existing code.\n",
    "\n",
    "There are a number of different NN's available, let's just hook into Scikit-learns Multi-Layer Perceptron for regression, that is an 'MLPRegressor'. \n",
    "\n",
    "Now, the data-set for training the MLP is really not well scaled, so we need to tweak a lot of parameters in the MLP just to get it to produce some sensible output: with out preprocessing and scaling of the input data, `X`, the MLP is really a bad choice of model for the job since it so easily produces garbage output. \n",
    "\n",
    "Try training the `mlp` regression model below, predict the value for Cyprus, and find the `score` value for the training set...just as we did for the linear and KNN models.\n",
    "\n",
    "Can the `MLPRegressor` score function be compared with the linear and KNN-scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Setup MLPRegressor\n",
    "mlp = MLPRegressor( hidden_layer_sizes=(10,), solver='adam', activation='relu', tol=1E-5, max_iter=100000, verbose=True)\n",
    "mlp.fit(X, y.ravel())\n",
    "\n",
    "# lets make a MLP regressor prediction and redo the plots\n",
    "y_pred_mlp = mlp.predict(M) \n",
    "\n",
    "plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_knn, \"b\")\n",
    "plt.plot(m, y_pred_mlp, \"k\")\n",
    "\n",
    "print(mlp.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try training the `mlp` regression model below, predict the value for Cyprus, and find the `score` value for the training set...just as we did for the linear and KNN models.\n",
    "\n",
    "We have trained the mlp model, which changes each time the code is run because it's trained by a neural network now.\n",
    "The neural network stops the training when it doesn't improve more than tol=0.000010 for 10 consecutive epochs.\n",
    "\n",
    "\n",
    "### Can the `MLPRegressor` score function be compared with the linear and KNN-scores?\n",
    "They can be compared since the score function is implemented in the same way. The result is way different from the score results of the linear and knn scores, which in this case means that the mlp model is worse at the prediction since the result (R^2) is negative. This means that the predicted values are performing worse than if you where to take the average value of the data as a prediction. (Læs lige op på det her?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [OPTIONAL] Qe) Neural Network with pre-scaling\n",
    "\n",
    "Now, the neurons in neural networks normally expects input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range the you put of the neuron will saturate to it's min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. To af fix to the problem in Qd) is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: try prescale data for the MPL...any better?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :-\n",
    "2018-12-18| CEF, initial.                  \n",
    "2019-01-24| CEF, spell checked and update. \n",
    "2019-01-30| CEF, removed reset -f, did not work on all PC's. \n",
    "2019-08-20| CEF, E19 ITMAL update. \n",
    "2019-08-26| CEF, minor mod to NN exercise.\n",
    "2019-08-28| CEF, fixed dataset dir issue, datapath\"../datasets\" changed to \"./datasets\".\n",
    "2020-01-25| CEF, F20 ITMAL update.\n",
    "2020-08-06| CEF, E20 ITMAL update, minor fix of ls to dir and added exception to datasets load, udpated figs paths.\n",
    "2020-09-24| CEF, updated text to R2, Qa exe.\n",
    "2020-09-28| CEF, updated R2 and theta extraction, use python attributes, moved revision table. Added comment about MLP.\n",
    "2021-01-12| CEF, updated Qe.\n",
    "2021-02-08| CEF, added ls for Mac/Linux to dir command cell. \n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2021-08-03| CEF, fixed ref to p21 => p.22.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2022-08-30| CEF, update to v1 changes.\n",
    "2023-08-30| CEF, minor table update for.\n",
    "2023-09-01| CEF, fixed ref to p.22 => p.28, changed LaTeX mbox and newcommand (VSCode error) to textrm/mathrm and renewcommand.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
