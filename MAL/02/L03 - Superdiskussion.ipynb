{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## L03 - Supergruppe diskussion\n",
    "\n",
    "The following assignment contains the summaries from the supergroup discussion.\n",
    "\n",
    "\n",
    "#### Resume: **Look at the Big Picture**\n",
    "\n",
    "1. Frame the problem\n",
    "\t- What exactly is the objective of making the machine learning model, how will it be used when finished. This determains algorithms to use, what performance you want and how precise it should be. Often the model will be used in a pipeline (series of processing components (maybe multiple machine learning models)).\n",
    "\t\n",
    "\t- Ask, what is the current solution, will give reference of the performance goal\n",
    "\n",
    "\t- From this you should be able to determain what model to use and how this model should learn\n",
    "\n",
    "2. Select a performance measure\n",
    "\t- Root Mean Square Error (RMSE)\n",
    "\t- Mean absoloute error\n",
    "\t- And so on...\n",
    "\n",
    "3. Check the assumptions\n",
    "\t- Check and verify the assumptions made so far\n",
    "\t- Check if the models fits in the pipeline\n",
    "\n",
    "#### Resume: **Get the Data**\n",
    "\n",
    "1. Create the workspace\n",
    "\t- Create a workspace for the machine learning code and datasheet\n",
    "\t- Python, Jupyter, NumPy, Pandas, MatplotLib, Scikit-Learn....\n",
    "\t- Think if you want to work in an isolated environment or not\n",
    "\t- Install all modules and dependencies\n",
    "\t- And start the different things (like Jupyter, create a notebook)\n",
    "\n",
    "2. Download the data\n",
    "\t- Typically from a database\n",
    "\n",
    "3. Look at the data structure\n",
    "\n",
    "4. Create a test set\n",
    "\t- Set aside part of the data\n",
    "\t- 20 percent\n",
    "\n",
    "\n",
    "\n",
    "#### Resume: **Explore and Visualize the Data to Gain Insights**\n",
    "\n",
    "When working with a dataset, it's a good idea to visualize the data to gain insights and identify \"data quirks\" that might need to be addressed before feeding the data to a machine learning algorithm. In the given example, geographical data is visualized using a scatterplot to identify patterns between latitude and longitude. Afterwards California housing prices are visualized to illustrate the correlation with population density and its housing pricing. To explore further correlations, the corr() method can be used.\n",
    "\n",
    "The correlation coefficient ranges from -1 to 1, where a value close to 1 indicates a strong correlation. The example provided by the book calculated a high correlation value of 0.69 between the median house value and income, suggesting that median house value tends to rise with median income. A value close to -1 indicates a strong negative correlation, while a value of 0 means that there is no linear correlation.\n",
    "\n",
    "Finally, different attribute combinations are experimented with, revealing new associations in the data.\n",
    "\n",
    "#### Resume: **Prepare the Data for Machine Learning Algorithms**\n",
    "\n",
    "When preparing data for an algorithm, it's useful to create functions so that transformations can be reused on new datasets in the future. Firstly, the data needs to be cleaned, meaning missing features need to be taken care of. This can be done by getting rid of the corresponding districts, removing the whole attribute, or setting the missing value to something (called imputation). Imputation could be zero, the mean, the median, and so on.\n",
    "\n",
    "Most machine learning algorithms don't perform well when their numerical attributes have very different scales. Therefore, feature scaling is applied to scale the dataset using the most common methods: min-max scaling and standardization.\n",
    "\n",
    "Custom transformers can be written to perform many of these operations, and Scikit-Learn also provides a pipeline class to help with this. Using these pipelines automates and ensures the process is executed in the right order. Lastly, in the chapter a pipeline to do all the described work is given.\n",
    "\n",
    "#### Resume: **Select and Train a Model**\n",
    "\n",
    "**Training and evaluating on the training set**\n",
    "\n",
    "The first model that will be trained and evaluated is LinearRegression.\n",
    "The model is then fitted with the training data and then it's tested\n",
    "with a specific selected part of the training data. \n",
    "The model works, but the data is very off, so the RMSE score for the whole\n",
    "training set will be calculated\n",
    "It shows that the model is underfitting, meaning that the model is to\n",
    "simple or that there's not enough features to make a good prediction.\n",
    "\n",
    "To try to solve this issue, there will be used a stronger model.\n",
    "DecisionTreeRegressor is chosen and it's fitted and evaluated, but\n",
    "it's fitted and tested on the exact same data and amount of data, \n",
    "which gives a RMSE score of 0.\n",
    "The model is very overfitted. \n",
    "\n",
    "**Better evaluation using Cross-Validation**\n",
    "\n",
    "A way to solve the issue above, it to use cross-validation methods.\n",
    "This way the testing data is split up, and the model is tested over \n",
    "several times. It also gives us a mean of the scores and the standard\n",
    "deviation.\n",
    "\n",
    "This method is then used on DecisionTreeRegressor and LinearRegression\n",
    "and the data suggest that DTR performs worse LR, because DTR is very\n",
    "overfitted. \n",
    "\n",
    "Last model to try is RandomForestRegressor, it works by training many\n",
    "DecisionTrees with random amount of subsets of the features. Then it\n",
    "means out their predictions and gives a complete prediction.\n",
    "This is called Ensemble Learning.\n",
    "\n",
    "The model is then fitted and tested with the cross-validation method and\n",
    "the data looks promising, but the mean score is very low and this\n",
    "indicates that the model is overfitting the training set. \n",
    "Solutions to overfitting is simplifying the model, constrain the model or\n",
    "get a lot more data. \n",
    "\n",
    "A good way to find a model, is to test a lot of different models, and then\n",
    "afterwards decide 3-5 models that look promissing and then start contrain\n",
    "the model or start adding a lot more data, if there's problems.\n",
    "\n",
    "#### Resume: **Fine-Tune Your Model**\n",
    "\n",
    "**Grid search**\n",
    "Instead of tweaking the hyperparameters manually, there's a tool called \n",
    "in Scikit-Learn called GridSearchCV. What this tool does, is it will\n",
    "evaluate what combination of hyperparamter values that is the best for \n",
    "the model. \n",
    "You'll need to tell it which hyperparameters you want to test and what\n",
    "their values will be.\n",
    "\n",
    "Doing this with this with RandonForestRegressor and some specific\n",
    "hyperparameters and values we get that the best combination is\n",
    "when \"max_features=6\" and \"n_estimators=30\". \n",
    "The RMSE score can also be found with this tool. \n",
    "\n",
    "**Randomized Search**\n",
    "If the hyperparameter space is large, randomized search is better than grid\n",
    "search. Grid search has predefined values and will test out the \n",
    "combinations of values. Randomized search will have a random values\n",
    "at each iteration, meaning that you can run 1000 iterations, where every\n",
    "hyperparameter value is different. \n",
    "\n",
    "\n",
    "**Ensemble Methods**\n",
    "Combining different models often gives a better outcome than just using\n",
    "one model.\n",
    "\n",
    "#### Resume: **Launch, Monitor, and Maintain Your System**\n",
    "\n",
    "--- \n",
    "\n",
    "Summary: \n",
    "It emphasizes the importance of thorough preparation for system launch, continuous monitoring, human evaluation, data quality monitoring, regular model training, and practical application of machine learning skills.\n",
    "\n",
    "--- \n",
    "\n",
    "More specific: \n",
    "\n",
    "- Launch Preparation: \n",
    "After approval, prepare for production by integrating production input data sources into the system and writing tests.\n",
    "\n",
    "- Monitoring: \n",
    "Implement monitoring code to regularly check system performance and trigger alerts for sudden breakage or performance degradation. Models tend to \"rot\" over time, so regular monitoring is essential.\n",
    "\n",
    "- Human Evaluation Pipeline: \n",
    "Incorporate a pipeline for human evaluation of system predictions, either through field experts or crowdsourcing platforms.\n",
    "\n",
    "- Input Data Quality: \n",
    "Monitor input data quality to catch issues like malfunctioning sensors or stale data early, especially crucial for online learning systems.\n",
    "\n",
    "- Regular Model Training: \n",
    "Automate model training using fresh data regularly to prevent severe performance fluctuations. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Resume: **Try It Out!**\n",
    "\n",
    "Encouragement to practice the machine learning process by selecting a dataset, going through the entire process from data preparation to model training, with platforms like Kaggle providing resources and a community for support.\n",
    "\n",
    "Emphasis on Overall Process: \n",
    "While machine learning algorithms are crucial, mastering the overall process, including data preparation, monitoring, and model training automation, is equally important.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
