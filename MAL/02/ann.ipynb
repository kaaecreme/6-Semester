{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Artificial Neural Networks as Universal Approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def GenerateSimpleData():\n",
    "    X = np.linspace(-10, 10, 100)\n",
    "    y = 2*np.tanh(2*X - 12) - 3*np.tanh(2*X - 4)  \n",
    "    y = 2*np.tanh(2*X + 2)  - 3*np.tanh(2*X - 4)   \n",
    "    X = X.reshape(-1, 1) # Scikit-algorithms needs matrix in (:,1)-format\n",
    "    return X,y\n",
    "\n",
    "X, y_true = GenerateSimpleData()\n",
    "plt.plot(X, y_true, \"r-.\")\n",
    "plt.legend([\"y_true\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\") \n",
    "plt.title(\"ANN, Groundtruth data simple\")\n",
    "           \n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qa)\n",
    "\n",
    "Below the model is fitted using the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(activation = 'tanh',      # activation function \n",
    "                   hidden_layer_sizes = [2], # layes and neurons in layers: one hidden layer with two neurons\n",
    "                   alpha = 1e-5,             # regularization parameter\n",
    "                   solver = 'lbfgs',         # quasi-Newton solver\n",
    "                   max_iter=10000,\n",
    "                   verbose = True)\n",
    "\n",
    "mlp.fit(X, y_true)\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, the true data and the predicted data is plotted in a graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y_true, \"b.\")\n",
    "plt.plot(X, y_pred, \"g-\")\n",
    "plt.legend([\"y_true\", \"y_pred\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data simple\")\n",
    "\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, both the weights and bias coefficients are printed, by accessing the attributes in the corresponding lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefs and intecepts\")\n",
    "print(f\"Weights: {mlp.coefs_[0]}\")\n",
    "print(f\"Bias: {mlp.intercepts_[0]}\\n\")\n",
    "print(f\"Weights: {mlp.coefs_[1]}\")\n",
    "print(f\"Bias: {mlp.intercepts_[1]}\")   \n",
    "        \n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb)\n",
    "\n",
    "Below is a drawing of the ANN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! MISSING DRAWING !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a specific mathematical formula is created for this model. \n",
    "\n",
    "The weights and biases have been extracted from the MLP from Qa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following weights and biases has been extracted from the mlp model from Qa\n",
    "w00 = mlp.coefs_[0][0][0]\n",
    "w01 = mlp.coefs_[0][0][1]\n",
    "\n",
    "b00 = mlp.intercepts_[0][0]\n",
    "b01 = mlp.intercepts_[0][1]\n",
    "\n",
    "w10 = mlp.coefs_[1][0][0]\n",
    "w11 = mlp.coefs_[1][1][0]\n",
    "\n",
    "b10 = mlp.intercepts_[1][0]\n",
    "\n",
    "# This is a generic mathematical formula of our ANN \n",
    "# ! ITS FROM GEMINI\n",
    "#y = w1 * tanh(w2 * x + b1) + w3 * tanh(w4 * x + b2) + b3\n",
    "\n",
    "# This is our specific mathematical \n",
    "y_math = w01 * np.tanh(w00 * X + b00) + w11 * np.tanh(w10 * X + b01) + b10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the mathematical function for the model, found in Qc, is plotted. \n",
    "\n",
    "The tanh function from numpy, and X, are used as inputs in the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y_true, \"b.\")\n",
    "plt.plot(X, y_pred, \"g-\")\n",
    "plt.plot(X, y_math, \"r-\")\n",
    "plt.legend([\"y_true\", \"y_pred\", \"y_math\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen on the plot above, all of the functions are completely similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and second half of the function is plotted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_math_first_part = w00 * np.tanh(w01 * X + b00)\n",
    "y_math_second_part = w10 * np.tanh(w11 * X + b01)\n",
    "\n",
    "plt.plot(X, y_math_first_part, \"b-.\")\n",
    "plt.plot(X, y_math_second_part, \"g-\")\n",
    "plt.legend([\"y_math_fp\", \"y_math_sp\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, y_math split up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are the first and second parts similar to a monotonic tanh activation function?**\n",
    "  \n",
    " Yes, both the first and second part are similar to a monotonic tanh activation function. \n",
    "  \n",
    "  -  Both of the plotted parts are monotonic like the tanh activation function, meaning each of them always increases or always decreases as the input values increase. \n",
    "\n",
    "  - Both parts also have the characteristic S-shaped Curve. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Explain the ability of the two-neuron network to be a general approximator for the input function**\n",
    "\n",
    "A general approximator can learn to approximate any kind of relationship between input and output data. \n",
    "\n",
    "In the same way a network with only two neurons can learn to do the same. Even though two neurons does not sound like a lot for a network, they still have the ability to, fx., add nonlinearity through activation functions. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qf)\n",
    "\n",
    "Now we change the data generator to a `sinc`-like function, which is a function that needs a NN with a higher capacity than the previous simple data.\n",
    "\n",
    "Extend the MLP with more neurons and more layers, and plot the result. Can you create a good approximation for the `sinc` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def GenerateSincData():\n",
    "    # A Sinc curve, approximation needs more neurons to capture the 'ringing'...\n",
    "    X = np.linspace(-3, 3, 1000) \n",
    "    y = np.sinc(X)\n",
    "    X = X.reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "mlp = MLPRegressor(activation = 'tanh',      # activation function \n",
    "                   hidden_layer_sizes = [10,10], # layes and neurons in layers: two hidden layers with ten neurons pr. layer\n",
    "                   alpha = 1e-5,             # regularization parameter\n",
    "                   solver = 'lbfgs',         # quasi-Newton solver\n",
    "                   max_iter=10000,\n",
    "                   verbose = True)\n",
    "\n",
    "mlp.fit(X, y_true)\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "X, y_true = GenerateSincData()\n",
    "plt.plot(X, y_true, \"r.\")\n",
    "plt.plot(X, y_pred, \"b-\")\n",
    "plt.legend([\"y_true\", \"y_pred\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data for Sinc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
