{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressesor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever library you want (except a linear regressor itself).\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* the `fit-predict` interface, and a $R^2$ score function,\n",
    "* one-dimensional output only,\n",
    "* loss function based on (R)MSE,\n",
    "* setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor, the signature of your `__init__` must include the named parameters `max_iter` and `eta0`,\n",
    "* the batch-gradient decent algorithm (GD),\n",
    "* constant or adaptive learning rate,\n",
    "* learning graphs,\n",
    "* stochastic gradient descent (SGD),\n",
    "* epochs vs iteations,\n",
    "* compare the numerical optimization with the Closed-form solution.\n",
    "\n",
    "### Qb: [OPTIONAL] Additional Concepts and Implementations\n",
    "\n",
    "And perhaps you could include (SHOULD/COULD):\n",
    "\n",
    "* (stochastic) mini-bach gradient decent, \n",
    "* early stopping,\n",
    "* interface to your bias and weights via `intercept_` and `coef_` attributes on your linear regressor `class`,\n",
    "* get/set functionality of your regressor, such that is fully compatible with other Scikit-learn algorithms, try it out in say a `cross_val_score()` call from Scikit-learn,\n",
    "* test in via the smoke tests at the end of this Notebook,\n",
    "* testing it on MNIST data, \n",
    "\n",
    "With the following no-nos (WONT):\n",
    "\n",
    "* no multi-linear regression,\n",
    "* no reuse of the Scikit-learn regressor\n",
    "* no `C/C++` optimized implementaion with a _thin_ Python interface (nifty, but to much work for now),\n",
    "* no copy-paste of code from other sources WITHOUT a clear cite/reference for your source.\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "Use mainly very low-dimensional data for testing, say the Iris data, since it might be very slow. Or create a simple low-dimensionality data generator.\n",
    "\n",
    "If you are brave, try your regressor on MNIST, and be prepared to tune on your learning rate, $\\eta$, since your regressor can behave badly (loss becomes higher and higher for each iteration).\n",
    "\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts).\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no  newlines in defs}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\def\\diff#1{\\mathrm{d}#1}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\byt{\\mathbf{y}\\st{true}}\n",
    "    \\def\\yt{y\\st{true}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "        } \n",
    "$$ \n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\byt =\n",
    "  \\ac{c}{\n",
    "     \\yt\\pown{1} \\\\\n",
    "     \\yt\\pown{2} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\yt\\pown{n} \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (appended one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} &\\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$ (with the additional element $w_0$ being the bias)\n",
    "\n",
    "$$\n",
    "\\ar{rll}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= y\\st{pred}\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni  & (\\bx\\powni~\\textrm{with bias})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part asuming one dimenesional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || y\\st{pred} \\powni     - y\\st{true}\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - y\\st{true}\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - y\\st{true}\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - y\\st{true}\\powni~\\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "  \\textrm{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                    &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\st{true}\\powni \\right)^2\\\\\n",
    "                    &= \\frac{1}{n} ||\\bX \\bw - \\byt~||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "     J &= \\frac{1}{2} ||\\bX \\bw - \\byt~||_2^2\\\\\n",
    "       &\\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "### Traning I: The Closed-form Solution\n",
    "\n",
    "* Finding the optimal weight in a _on-step_ analytic expression \n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\byt\n",
    "$$\n",
    "\n",
    "\n",
    "### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\byt \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD in 2-D\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test-suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini smoke test for your linear regressor...\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "def PrintOutput(msg, pre_msg, ex=None, color=\"\", filestream=sys.stdout):\n",
    "    #BLACK    =\"\\033[0;30m\"\n",
    "    #BLUE     =\"\\033[0;34m\"\n",
    "    #LBLUE    =\"\\033[1;34m\"\n",
    "    #RED      =\"\\033[0;31m\"\n",
    "    #LRED     =\"\\033[1;31m\"\n",
    "    #GREEN    =\"\\033[0;32m\"\n",
    "    #LGREEN   =\"\\033[1;32m\"\n",
    "    #YELLOW   =\"\\033[0;33m\"\n",
    "    #LYELLOW  =\"\\033[1;33m\"\n",
    "    #PURPLE   =\"\\033[0;35m\"\n",
    "    #LPURPLE  =\"\\033[1;35m\"\n",
    "    #CYAN     =\"\\033[0;36m\"\n",
    "    #LCYAN    =\"\\033[1;36m\"\n",
    "    #BROWN    =\"\\033[0;33m\"\n",
    "    #DGRAY    =\"\\033[1;30m\"\n",
    "    #LGRAY    =\"\\033[0;37m\"\n",
    "    #WHITE    =\"\\033[1;37m\"\n",
    "    #NC       =\"\\033[0m\"\n",
    "    color_end = \"\\033[0m\" if color!=\"\" else \"\"\n",
    "    if ex is not None:\n",
    "        msg += f\"\\n   EXCEPTION: {ex} ({type(ex)})\"\n",
    "    print(f\"{color}{pre_msg}{msg}{color_end}\", file=filestream)\n",
    "\n",
    "def Warn(msg, ex=None):\n",
    "    PrintOutput(msg, \"WARNING: \", ex, \"\\033[1;33m\")\n",
    "\n",
    "def Err(msg, ex=None):\n",
    "    PrintOutput(msg, \"ERROR: \", ex, \"\\033[1;31m\" )\n",
    "    exit(-1)\n",
    "\n",
    "def Info(msg):\n",
    "    PrintOutput(msg, \"\", None, \"\\033[1;35m\")\n",
    "\n",
    "def SimplePrintMatrix(x, label=\"\", precision=12):\n",
    "    # default simple implementation, may be overwritten by a libitmal function later..\n",
    "    print(f\"{label}{' ' if len(label)>0 else ''}{x}\")\n",
    "\n",
    "def SimpleAssertInRange(x, expected, eps):\n",
    "    #assert isinstance(x, numpy.ndarray)\n",
    "    #assert isinstance(expected, numpy.ndarray)\n",
    "    #assert x.ndim==1 and expected.ndim==1\n",
    "    #assert x.shape==expected.shape\n",
    "    assert eps>0\n",
    "    assert numpy.allclose(x, expected, eps) # should rtol or atol be set to eps?\n",
    "\n",
    "def GenerateData():\n",
    "    X = numpy.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = numpy.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "def TestMyLinReg():\n",
    "    X, y = GenerateData()\n",
    "\n",
    "    try:\n",
    "        # assume that your regressor class is named 'MyLinReg', please update/change\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor has another name, than 'MyLinReg', please change the name in this smoke test\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor = MyLinReg(max_iter=200, eta0=0.4)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ (with two parameters, see call above\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor.fit(X, y)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not fit\", ex)\n",
    "\n",
    "    try:\n",
    "        y_pred = regressor.predict(X)\n",
    "        Info(f\"y_pred = {y_pred}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not predict\", ex)\n",
    "\n",
    "    try:\n",
    "        score  = regressor.score(X, y)\n",
    "        Info(f\"SCORE = {score}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails in the score call\", ex)\n",
    "\n",
    "    try:\n",
    "        w    = None # default\n",
    "        bias = None # default\n",
    "        try:\n",
    "            w = regressor.coef_\n",
    "            bias = regressor.intercept_\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Warn(\"your regressor has no coef_/intercept_ atrributes, trying Weights() instead..\", ex)\n",
    "        try:\n",
    "            if w is None:\n",
    "                w = regressor.Weights() # maybe a Weigths function is avalible on you model?\n",
    "                try:\n",
    "                    assert w.ndim == 1,     \"can only handle vector like w's for now\"\n",
    "                    assert w.shape[0] >= 2, \"expected length of to be at least 2, that is one bias one coefficient\"\n",
    "                    bias = w[0]\n",
    "                    w = w[1:]\n",
    "                except Exception as ex:\n",
    "                    w = None\n",
    "                    Err(\"having a hard time concantenating our bias and coefficients, giving up!\", ex)\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Err(\"your regressor also has no Weights() function, giving up!\", ex)\n",
    "        Info(f\"bias         = {bias}\")\n",
    "        Info(f\"coefficients = {w}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails during extraction of bias and weights (but is a COULD)\", ex)\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import PrintMatrix\n",
    "    except Exception as ex:\n",
    "        PrintMatrix = SimplePrintMatrix # fall-back\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import AssertInRange\n",
    "    except Exception as ex:\n",
    "        AssertInRange = SimpleAssertInRange # fall-back\n",
    "        Warn(\"could not import AssertInRange from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        if w is not None:\n",
    "            if bias is not None:\n",
    "                w = numpy.concatenate(([bias], w)) # re-concat bias an coefficients, may be incorrect for your implementation!\n",
    "            # TEST VECTOR:\n",
    "            w_expected = numpy.array([4.046879011698, 1.880121487278])\n",
    "            PrintMatrix(w,          label=\"\\tw         =\", precision=12)\n",
    "            PrintMatrix(w_expected, label=\"\\tw_expected=\", precision=12)\n",
    "            eps = 1E-3 # somewhat big epsilon, allowing some slack..\n",
    "            AssertInRange(w, w_expected, eps)\n",
    "            Info(\"well, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\")\n",
    "        else:\n",
    "            Warn(\"cannot test due to missing w information\")\n",
    "    except Exception as ex:\n",
    "        Err(\"mini-smoketest on your regressor failed\", ex)\n",
    "\n",
    "Warn(\"This mini smoke test is currently untested, please modify...\")\n",
    "TestMyLinReg()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-testing\n",
    "\n",
    "Or perhaps do a comparison test with the SGD regressor in Scikit-learn using the next smoke-test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\", dataloaders.IRIS_GetDataSet, 1E-2), (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(max_iter=1000, eta0=eta0)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            Info(f\"y_pred_test={y_pred_test}\")\n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {r2:0.3f}\")\n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release.\n",
    "2023-02-28| CEF, fix a few issues related to import from libitmal, added Info and color output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (demo)",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
