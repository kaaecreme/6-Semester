{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Your Own Linear Regressesor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever library you want (except a linear regressor itself).\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* the `fit-predict` interface, and a $R^2$ score function,\n",
    "* one-dimensional output only,\n",
    "* loss function based on (R)MSE,\n",
    "* setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor, the signature of your `__init__` must include the named parameters `max_iter` and `eta0`,\n",
    "* the batch-gradient decent algorithm (GD),\n",
    "* constant or adaptive learning rate,\n",
    "* learning graphs,\n",
    "* stochastic gradient descent (SGD),\n",
    "* epochs vs iteations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class MyLinearRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, max_iter=1000, eta0=0.01, gd_type='batch'):\n",
    "        self.max_iter = max_iter # Number of iterations\n",
    "        self.eta0 = eta0 # Learning rate\n",
    "        self.gd_type = gd_type # Which fit should be run\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y_true):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)  # Initialize weights with zeros\n",
    "        self.bias = 0  # Initialize bias with zero\n",
    "\n",
    "        #Gemini chatbot has been used to give us the idea to split the fit up\n",
    "        #So the linear regressor can be initialized with either batch or stochastic gradiant descent \n",
    "    \n",
    "        if self.gd_type == 'batch':\n",
    "            self.fit_batch_gd(X, y_true)\n",
    "        elif self.gd_type == 'sgd':\n",
    "            self.fit_sgd(X, y_true)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid gradient descent type\")\n",
    "        \n",
    "    #This method has been created by using Gemini chatbot:\n",
    "    def fit_batch_gd(self, X, y_true):\n",
    "        m, n = X.shape\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # Calculate predictions\n",
    "            y_pred = self.predict(X)\n",
    "\n",
    "            # Calculate errors\n",
    "            errors = y_pred - y_true\n",
    "\n",
    "            # Update weights and bias using gradient descent\n",
    "            self.weights -= self.eta0 * (1/m) * np.dot(X.T, errors)\n",
    "            self.bias -= self.eta0 * (1/m) * np.sum(errors)\n",
    "            \n",
    "    #This method has been created by using Gemini chatbot:  \n",
    "    def fit_sgd(self, X, y_true):\n",
    "        m, n = X.shape\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            for i in range(m):  # Iterate over each example\n",
    "                random_index = np.random.randint(0, m)  # Pick a random sample\n",
    "                x_i = X[random_index:random_index + 1]  # Select the ith sample\n",
    "                y_i = y_true[random_index:random_index + 1]\n",
    "\n",
    "                # Calculate predictions\n",
    "                y_pred = self.predict(x_i)\n",
    "\n",
    "                # Calculate errors\n",
    "                errors = y_pred - y_i\n",
    "\n",
    "                # Update weights and bias using gradient descent\n",
    "                self.weights -= self.eta0 * np.dot(x_i.T, errors)\n",
    "                self.bias -= self.eta0 * np.sum(errors)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights) + self.bias\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return r2_score(y, self.predict(X))\n",
    "\n",
    "    def RMSE(self, y_pred, y_true):\n",
    "        self.checkInputSameShape(y_pred, y_true)\n",
    "          \n",
    "        # Calculate squared differences (L2)\n",
    "        squared_diff = (y_true - y_pred) ** 2\n",
    "        \n",
    "        # Compute mean of squared differences\n",
    "        mean_squared_diff = np.mean(squared_diff)\n",
    "        \n",
    "        # Take square root to obtain RMSE\n",
    "        rmse_value = np.sqrt(mean_squared_diff)\n",
    "        \n",
    "        return rmse_value\n",
    "    \n",
    "    # From our own previous assignment\n",
    "    def checkInputSameShape(self, y_pred, y_true):\n",
    "        assert y_pred.shape == y_true.shape, \"Shape of input is not equal!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc: Testing and Test Data\n",
    "\n",
    "In the following exercise the Iris data has been used to test the regressors by fitting them with the data. Following this the RMSE score will be found and compared between the batch- and stochastic gradiant descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y_true = iris.target\n",
    "\n",
    "# Init the linear regressor, batch and sgd:\n",
    "regressor_batch = MyLinearRegressor(gd_type=\"batch\")\n",
    "regressor_sgd = MyLinearRegressor(gd_type=\"sgd\")\n",
    "\n",
    "# Fit linear regressor with iris data:\n",
    "regressor_batch.fit(X, y_true)\n",
    "regressor_sgd.fit(X,y_true)\n",
    "\n",
    "# Make prediction for batch:\n",
    "y_pred_batch = regressor_batch.predict(X)\n",
    "rmse_batch = regressor_batch.RMSE(y_pred_batch, y_true)\n",
    "print(f\"RMSE score from the batch regressor: {rmse_batch}\")\n",
    "\n",
    "# Make prediction for sgd:\n",
    "y_pred_sgd = regressor_sgd.predict(X)\n",
    "rmse_sgd = regressor_sgd.RMSE(y_pred_sgd, y_true)\n",
    "print(f\"RMSE score from the sgd regressor: {rmse_sgd}\")\n",
    "\n",
    "print(\"\\nIt is seen that the batch regressor in most cases has a lower RMSE score than the sgd regressor\")\n",
    "\n",
    "print(\"\\nThis can also be visually represented by plotting the predictions and comparing these with the true value\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Batch Regressor\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_true, y_pred_batch, color='blue', label='Batch Predictions')\n",
    "plt.plot(y_true, y_true, color='red', label='Ideal (y_true = y_pred)')  # Ideal line\n",
    "plt.xlabel('True Values (y_true)')\n",
    "plt.ylabel('Predicted Values (y_pred)')\n",
    "plt.title('Batch Linear Regressor')\n",
    "plt.legend()\n",
    "plt.xticks([0, 1, 2]) # Three flower types\n",
    "\n",
    "# SGD Regressor\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_true, y_pred_sgd, color='green', label='SGD Predictions')\n",
    "plt.plot(y_true, y_true, color='red', label='Ideal (y_true = y_pred)')  # Ideal line\n",
    "plt.xlabel('True Values (y_true)')\n",
    "plt.ylabel('Predicted Values (y_pred)')\n",
    "plt.title('SGD Linear Regressor')\n",
    "plt.legend()\n",
    "plt.xticks([0, 1, 2]) # Three flower types\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` is a constructor for our class, where its parameter are the number of iteration to go through, the learning rate and which type of model we want to use.\n",
    "\n",
    "The `fit` method is where we implement the generel fit structure, first we split up the dataset X, into m and n. M is the number of training examples and n is the number of features. \n",
    "\n",
    "Then we create a weight matrix that is the size of n, but it's filled with 0. One of the reasons this is done, is because we don't want the model to favour any specific feature in the first training phase. We also initialize the bias with 0, and this value will change during the training. \n",
    "\n",
    "After this we'll look at what model the object is initialized to use and use that specific fit method\n",
    "\n",
    "`fit_batch_gd` is the fit method for batch gd. First we find the number of training examples m, then we run the code that is about to be descriped in a for loop, where it'll run for the amount of iterations that's been specified. \n",
    "\n",
    "First we'll use the predict method on our data and save this as y_pred. Then the y_true will be subtracted from the y_pred and this will result in the amount of errors in the prediction. \n",
    "\n",
    "Based on these errors the weights and bias' will be updated based on gradient descent.\n",
    "\n",
    "`fit_sgd` is the fit method for the stochastic gd. Again we extract the number of training examples m, then this time we'll have a for loop inside of a for loop. The first for loop is the same as the previous one, it'll run for the amount of iterations as specified. The second for loop will run for the amount of training examples in the dataset. \n",
    "\n",
    "First there'll be picked a random sample, then the X sample value and y_true sample value will be picked based on the random sample. Then the predict method will be used of X sample value and the return value will be y_pred. \n",
    "\n",
    "Now the y_true sample will be deducted from the y_pred to find the error between the two samples and the weights and bias' will be changed based on gradient descent. \n",
    "\n",
    "`predict` is the predict method for the model. It'll use a dataset X, weights and bias to try and predict the y value. It'll do this by taking dot product between dataset matrix X and the weight matrix and then add the bias to the result. \n",
    "\n",
    "`score` is a method that uses the r2score-method. the r2score-method will compare y_true and y_pred and then give a r2 score. So the higher the score, the better our model has predicted the results. \n",
    "\n",
    "`RMSE` this method is used to calculate the loss of the model, the closer it is to 0, the better. First we're checking if y_pred and y_true are the same shape, this is done with the method `checkInputSameShape`.\n",
    "\n",
    "Afterwards we first need to calculate the squared difference between y_pred and y_true. Then we get the mean of this squared difference. Now we have the MSE score, but we would like the RMSE score, so we're taking the rootsquare of the MSE value and now we have the RMSE score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test-suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BATCH:\n",
      "\u001b[1;35my_pred = [5.61498307 6.75547417 4.04730904 5.18372293]\u001b[0m\n",
      "\u001b[1;35mSCORE = 0.495005642955747\u001b[0m\n",
      "\u001b[1;35mbias         = 4.046878960223288\u001b[0m\n",
      "\u001b[1;35mcoefficients = [1.88012155]\u001b[0m\n",
      "\tw         =[4.046878960223 1.880121547044]\n",
      "\tw_expected=[4.046879011698 1.880121487278]\n",
      "\u001b[1;35mwell, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\u001b[0m\n",
      "\n",
      "Results for SGD:\n",
      "\u001b[1;35my_pred = [5.49975442 6.79557822 3.71856618 5.00975747]\u001b[0m\n",
      "\u001b[1;35mSCORE = 0.4748343231805191\u001b[0m\n",
      "\u001b[1;35mbias         = 3.718077524275512\u001b[0m\n",
      "\u001b[1;35mcoefficients = [2.1361905]\u001b[0m\n",
      "\tw         =[3.718077524276 2.136190504768]\n",
      "\tw_expected=[4.046879011698 1.880121487278]\n",
      "\u001b[1;31mERROR: mini-smoketest on your regressor failed\n",
      "   EXCEPTION: x=3.718077524275512 is not within the range [4.045879011697999; 4.047879011698] for eps=0.001, got eps=0.3288014874224876 (<class 'AssertionError'>)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Mini smoke test for your linear regressor...\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "def PrintOutput(msg, pre_msg, ex=None, color=\"\", filestream=sys.stdout):\n",
    "    #BLACK    =\"\\033[0;30m\"\n",
    "    #BLUE     =\"\\033[0;34m\"\n",
    "    #LBLUE    =\"\\033[1;34m\"\n",
    "    #RED      =\"\\033[0;31m\"\n",
    "    #LRED     =\"\\033[1;31m\"\n",
    "    #GREEN    =\"\\033[0;32m\"\n",
    "    #LGREEN   =\"\\033[1;32m\"\n",
    "    #YELLOW   =\"\\033[0;33m\"\n",
    "    #LYELLOW  =\"\\033[1;33m\"\n",
    "    #PURPLE   =\"\\033[0;35m\"\n",
    "    #LPURPLE  =\"\\033[1;35m\"\n",
    "    #CYAN     =\"\\033[0;36m\"\n",
    "    #LCYAN    =\"\\033[1;36m\"\n",
    "    #BROWN    =\"\\033[0;33m\"\n",
    "    #DGRAY    =\"\\033[1;30m\"\n",
    "    #LGRAY    =\"\\033[0;37m\"\n",
    "    #WHITE    =\"\\033[1;37m\"\n",
    "    #NC       =\"\\033[0m\"\n",
    "    color_end = \"\\033[0m\" if color!=\"\" else \"\"\n",
    "    if ex is not None:\n",
    "        msg += f\"\\n   EXCEPTION: {ex} ({type(ex)})\"\n",
    "    print(f\"{color}{pre_msg}{msg}{color_end}\", file=filestream)\n",
    "\n",
    "def Warn(msg, ex=None):\n",
    "    PrintOutput(msg, \"WARNING: \", ex, \"\\033[1;33m\")\n",
    "\n",
    "def Err(msg, ex=None):\n",
    "    PrintOutput(msg, \"ERROR: \", ex, \"\\033[1;31m\" )\n",
    "    exit(-1)\n",
    "\n",
    "def Info(msg):\n",
    "    PrintOutput(msg, \"\", None, \"\\033[1;35m\")\n",
    "\n",
    "def SimplePrintMatrix(x, label=\"\", precision=12):\n",
    "    # default simple implementation, may be overwritten by a libitmal function later..\n",
    "    print(f\"{label}{' ' if len(label)>0 else ''}{x}\")\n",
    "\n",
    "def SimpleAssertInRange(x, expected, eps):\n",
    "    #assert isinstance(x, numpy.ndarray)\n",
    "    #assert isinstance(expected, numpy.ndarray)\n",
    "    #assert x.ndim==1 and expected.ndim==1\n",
    "    #assert x.shape==expected.shape\n",
    "    assert eps>0\n",
    "    assert numpy.allclose(x, expected, eps) # should rtol or atol be set to eps?\n",
    "\n",
    "def GenerateData():\n",
    "    X = numpy.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = numpy.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "def TestMyLinReg(_gd_type = \"batch\"):\n",
    "    X, y = GenerateData()\n",
    "\n",
    "    try:\n",
    "        # assume that your regressor class is named 'MyLinReg', please update/change\n",
    "        regressor = MyLinearRegressor(gd_type=_gd_type)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor has another name, than 'MyLinReg', please change the name in this smoke test\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor = MyLinearRegressor(max_iter=1000, eta0=0.1, gd_type=_gd_type)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ (with two parameters, see call above\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor.fit(X, y)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not fit\", ex)\n",
    "\n",
    "    try:\n",
    "        y_pred = regressor.predict(X)\n",
    "        Info(f\"y_pred = {y_pred}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not predict\", ex)\n",
    "\n",
    "    try:\n",
    "        score  = regressor.score(X, y)\n",
    "        Info(f\"SCORE = {score}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails in the score call\", ex)\n",
    "\n",
    "    try:\n",
    "        w    = None # default\n",
    "        bias = None # default\n",
    "        try:\n",
    "            w = regressor.weights\n",
    "            bias = regressor.bias\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Warn(\"your regressor has no coef_/intercept_ atrributes, trying Weights() instead..\", ex)\n",
    "        try:\n",
    "            if w is None:\n",
    "                w = regressor.Weights() # maybe a Weigths function is avalible on you model?\n",
    "                try:\n",
    "                    assert w.ndim == 1,     \"can only handle vector like w's for now\"\n",
    "                    assert w.shape[0] >= 2, \"expected length of to be at least 2, that is one bias one coefficient\"\n",
    "                    bias = w[0]\n",
    "                    w = w[1:]\n",
    "                except Exception as ex:\n",
    "                    w = None\n",
    "                    Err(\"having a hard time concantenating our bias and coefficients, giving up!\", ex)\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Err(\"your regressor also has no Weights() function, giving up!\", ex)\n",
    "        Info(f\"bias         = {bias}\")\n",
    "        Info(f\"coefficients = {w}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails during extraction of bias and weights (but is a COULD)\", ex)\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import PrintMatrix\n",
    "    except Exception as ex:\n",
    "        PrintMatrix = SimplePrintMatrix # fall-back\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import AssertInRange\n",
    "    except Exception as ex:\n",
    "        AssertInRange = SimpleAssertInRange # fall-back\n",
    "        Warn(\"could not import AssertInRange from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        if w is not None:\n",
    "            if bias is not None:\n",
    "                w = numpy.concatenate(([bias], w)) # re-concat bias an coefficients, may be incorrect for your implementation!\n",
    "            # TEST VECTOR:\n",
    "            w_expected = numpy.array([4.046879011698, 1.880121487278])\n",
    "            PrintMatrix(w,          label=\"\\tw         =\", precision=12)\n",
    "            PrintMatrix(w_expected, label=\"\\tw_expected=\", precision=12)\n",
    "            eps = 1E-3 # somewhat big epsilon, allowing some slack..\n",
    "            AssertInRange(w, w_expected, eps)\n",
    "            Info(\"well, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\")\n",
    "        else:\n",
    "            Warn(\"cannot test due to missing w information\")\n",
    "    except Exception as ex:\n",
    "        Err(\"mini-smoketest on your regressor failed\", ex)\n",
    "\n",
    "print(\"Results for BATCH:\")\n",
    "TestMyLinReg(\"batch\")\n",
    "\n",
    "print(\"\\nResults for SGD:\")\n",
    "TestMyLinReg(\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
