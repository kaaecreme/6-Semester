{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "## L08 - Generalization error\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/dl_generalization_error.png\" alt=\"WARNING: could not get image from server.\" style=\"height:500px\">\n",
    "\n",
    "### Qa) On Generalization Error\n",
    "\n",
    "A detailed description of figure 5.3 is written below, diveded into given concepts from the assignment.\n",
    " \n",
    "\n",
    "__<u>Training/Generalization Error</u>__\n",
    "\n",
    "In the figure, the __training error__ is displayed by the dotted blue line. It represents the error / loss observed during the models training phase, and is calculated based on the difference between the models predictions and the actual values in the dataset, showing how well the model understands this pattern.\n",
    "\n",
    "__Generalization error__, shown by the green line on the graph, indicates the error / loss AFTER the trained model has been evaluated. This could be on data not used during the traning phase. It therefore measures how well the model can make predictions on new and unseen data.\n",
    "\n",
    "__<u>Underfit / overfit zone</u>__\n",
    "\n",
    "In the __underfit zone/regime__, it represents a situation where the model's capacity is too low, also seen to the left in the graph. In this zone, both the traning and generalization error are too high, and the model fails to learn on both trained and unseen data.\n",
    "\n",
    "The __Overfit zone/regime__ represents sutations where the capacity is too high, which might lead it to fitting ie. on the noise in the training data(meaning the model understands the patterns in the training data very well). The training error is thereby low, but the generation error is too high. \n",
    "\n",
    "__<u>Generalization gab</u>__\n",
    "\n",
    "The __generalization gab__ represents the difference between the training and generalization error. This is seen to the right in the graph when the capacity is being increased, where the gap size outweights the decrease in training error (leads to overfitting!)\n",
    "\n",
    "__<u>Optimal capactity</u>__\n",
    "\n",
    "The __optimal capacity__ represents a 'sweet spot', where the model greatly balances between being underfitted and overfitted. This is because of the model being able to find the patterns in the data without overfitting, meaning a good capacity value has been chosen to fit both unseen and trained data.\n",
    "\n",
    "__<u>The axes</u>__\n",
    "\n",
    "The y-axis represents the error / loss by the model, where the x-axis represents the model's capacity / complexity. Increasing a models capacity gives the model a better understand of the underlying patterns in the data, which is seen on the graph by the decrease in error. Increasing it too much, it starts overfitting, as previously explains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) A MSE-Epoch/Error Plot\n",
    "\n",
    "Now we will look into the SGD model for fitting polynomial (__polynomial regression__), described simialary in [HOML] (\"Polynomial Regression\" + \"Learning Curves\").  \n",
    "\n",
    "The code will be reviewed, each part by itself, and important points will be descriped. \n",
    "\n",
    "__<u>Part I</u>__\n",
    "\n",
    "Once again, data is being generated by the `GenerateData` function, also adding some noise to the dataset. We split the data into training and validation sets by using the `test_train_split` function. By using a 90 degree polynomial for the polynomial regression, we produce a model with a very high capacity. The `PolynomialFeatures` are added into a pipeline, afterwareds scaled by a `StandardScaler` to ensure that each feature has a mean of 0 and standard deviation of 1 (as done in previous assignments). Lastly both the training and validation sets are being transformed by the pipeline we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part I)\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def GenerateData():\n",
    "    m = 100\n",
    "    X = 6 * np.random.rand(m, 1) - 3\n",
    "    y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData()\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split( \\\n",
    "        X[:50], y[:50].ravel(), \\\n",
    "        test_size=0.5, \\\n",
    "        random_state=10)\n",
    "\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"X_val  .shape=\",X_val.shape)\n",
    "print(\"y_train.shape=\",y_train.shape)\n",
    "print(\"y_val  .shape=\",y_val.shape)\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled   = poly_scaler.transform(X_val)\n",
    "\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "plt.plot(X, y, \"b.\", label=\"All X-y Data\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18, )\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n",
    "\n",
    "print('OK')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<u>Part II</u>__\n",
    "\n",
    "The `Train` function trains our SGD regressor for a specified number of epochs, defined by `n_epochs` in the codeblock.\n",
    "\n",
    "An __epoch__ is a one complete pass through our training data set in the training phase, basically one iteration where it updates the model's parameters by using SGD.\n",
    "\n",
    "We use this function to train our model, and returning its result in the `tran_errors` and `val_errors` arrays. For each epoch, here set to 500, the model is being trained with a max iteration of 1, meaning  the algorithm will only update the model's parameters once (with a random selected sample from the training set), rather than processing the entire dataset.\n",
    "\n",
    "Lasty by setting `verbose` to true, we print out the `epoch`, `mse_train` and `mse_val`. The last two are metrics that we use to evaluate how the model performs (seperately) on both the training and validation sets. With a lower MSE value, it indicated a better performance. The `mse_train` tells us how well the model fits to the training data, and the `mse_val` tells us how well it generalizes on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part II)\n",
    "\n",
    "def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False):\n",
    "    print(\"Training...n_epochs=\",n_epochs)\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    sgd_reg = SGDRegressor(max_iter=1,\n",
    "                           penalty=None,\n",
    "                           eta0=0.0005,\n",
    "                           warm_start=True,\n",
    "                           early_stopping=False,\n",
    "                           learning_rate=\"constant\", \n",
    "                           tol=-float(0), \n",
    "                           random_state=42)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        sgd_reg.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_predict = sgd_reg.predict(X_train)\n",
    "        y_val_predict   = sgd_reg.predict(X_val)\n",
    "\n",
    "        mse_train=mean_squared_error(y_train, y_train_predict)\n",
    "        mse_val  =mean_squared_error(y_val  , y_val_predict)\n",
    "\n",
    "        train_errors.append(mse_train)\n",
    "        val_errors  .append(mse_val)\n",
    "        if verbose:\n",
    "            print(f\"  epoch={epoch:4d}, mse_train={mse_train:4.2f}, mse_val={mse_val:4.2f}\")\n",
    "\n",
    "    return train_errors, val_errors\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = Train(X_train_poly_scaled, y_train, X_val_poly_scaled, y_val, n_epochs, True)\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<u>Part III</u>__\n",
    "\n",
    "The last codeblock is determing the best model based on the epoch with the lowest validation RMSE (both the `best_epoch` and `best_val_rmse`). We plot the values of both the training and validation dataset, and the best model is highlighted on the grapth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part III)\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(train_errors), \"b--\", linewidth=2, label=\"Training set\")\n",
    "plt.plot(np.sqrt(val_errors), \"g-\", linewidth=3, label=\"Validation set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc)  Early Stopping\n",
    "\n",
    "Potentially we could implement early stopping to prevent overfitting. We will look at the model's performance on our validation set during training, and then stop when the performance starts to get worse (when we observe an increase in the validation error (`val_error`) compared to the `best_val_error`)\n",
    "\n",
    "Below is some siple pseudo code that could implement early stopping.\n",
    "\n",
    "```python\n",
    "best_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(n_epochs):  \n",
    "    if val_error < best_val_error:\n",
    "        best_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    if val_error > best_val_error:\n",
    "        print(\"Stopping training: Validation error increased, training stopped.\")\n",
    "        break \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Explain the Polynomial RMSE-Capacity plot\n",
    "\n",
    "In the generated plot, the x-axis _Capacity_ represents the capacity / complexity, and the y-axis _RMSE_ / error of the model.\n",
    "\n",
    "It is seen on the plot, that as we increase the capacity, both the training and validation RMS decreases at the beginning. This is due to the model becomming more and more complex as the capacity increases, and it's able to better fit the training data.\n",
    "\n",
    "However, around a degree of 3, we see that the validation RMSE starts to increase, even though the training RMSE still falls. This is because the model is becomming too complex and starts overfitting the data (i.e. starts to capture noise). Increasing the capacity even further, we see on the plot that the overfitting increases. The training RMSE continues to fall, as it's getting good at fitting noise in the training data (it becomes too specific for the given training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and review this code\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData():\n",
    "    n_samples = 30\n",
    "    #degrees = [1, 4, 15]\n",
    "    degrees = range(1,8)\n",
    "\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y, degrees\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, degrees  = GenerateData()\n",
    "\n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "capacities, rmses_training, rmses_validation= [], [], []\n",
    "for i in range(len(degrees)):\n",
    "    d=degrees[i]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    \n",
    "    Z = X[:, np.newaxis]\n",
    "    pipeline.fit(Z, y)\n",
    "    \n",
    "    p = pipeline.predict(Z)\n",
    "    train_rms = mean_squared_error(y,p)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, Z, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    score_mean = -scores.mean()\n",
    "    \n",
    "    rmse_training=sqrt(train_rms)\n",
    "    rmse_validation=sqrt(score_mean)\n",
    "    \n",
    "    print(f\"  degree={d:4d}, rmse_training={rmse_training:4.2f}, rmse_cv={rmse_validation:4.2f}\")\n",
    "    \n",
    "    capacities      .append(d)\n",
    "    rmses_training  .append(rmse_training)\n",
    "    rmses_validation.append(rmse_validation)\n",
    "    \n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(capacities, rmses_training,  \"b--\", linewidth=2, label=\"training RMSE\")\n",
    "plt.plot(capacities, rmses_validation,\"g-\",  linewidth=2, label=\"validation RMSE\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Capacity\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When increasing the model capacity above 10, we notice on the below plot an exponentially growing overfitting. As explained in the `capacity_under_overfitting.ipynb` exercises, this is due to the model overfitting the training data and becomes bad at predicting new data. (The same happened in the Qa+b exercise when we set the degrees too high, shown in the first three plots) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and review this code\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData():\n",
    "    n_samples = 30\n",
    "    #degrees = [1, 4, 15]\n",
    "    degrees = range(1,11)\n",
    "\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y, degrees\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, degrees  = GenerateData()\n",
    "\n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "capacities, rmses_training, rmses_validation= [], [], []\n",
    "for i in range(len(degrees)):\n",
    "    d=degrees[i]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    \n",
    "    Z = X[:, np.newaxis]\n",
    "    pipeline.fit(Z, y)\n",
    "    \n",
    "    p = pipeline.predict(Z)\n",
    "    train_rms = mean_squared_error(y,p)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, Z, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    score_mean = -scores.mean()\n",
    "    \n",
    "    rmse_training=sqrt(train_rms)\n",
    "    rmse_validation=sqrt(score_mean)\n",
    "    \n",
    "    print(f\"  degree={d:4d}, rmse_training={rmse_training:4.2f}, rmse_cv={rmse_validation:4.2f}\")\n",
    "    \n",
    "    capacities      .append(d)\n",
    "    rmses_training  .append(rmse_training)\n",
    "    rmses_validation.append(rmse_validation)\n",
    "    \n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(capacities, rmses_training,  \"b--\", linewidth=2, label=\"training RMSE\")\n",
    "plt.plot(capacities, rmses_validation,\"g-\",  linewidth=2, label=\"validation RMSE\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Capacity\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.517px",
    "left": "1228px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
